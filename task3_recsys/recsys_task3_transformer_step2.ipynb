{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# from tokenizers.implementations import BertWordPieceTokenizer\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from itertools import chain\n",
    "from collections import Counter\n",
    "import gc\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import mlflow\n",
    "import torch.optim as optim\n",
    "import mlflow.pytorch\n",
    "from torchmetrics import F1Score as F1\n",
    "from tqdm import tqdm\n",
    "\n",
    "train_df = pd.read_parquet(\"task3_recsys/df_for_transformer_TRAIN.parquet\")\n",
    "test_df = pd.read_parquet(\"task3_recsys/df_for_transformer_TEST.parquet\")\n",
    "tokenizer = PreTrainedTokenizerFast(tokenizer_file=\"task3_recsys/models/tokenizer/vocab_256_bpe.json\") # наиболее адекватное распределение по сравнению с большим кол-вом токенов\n",
    "special_tokens = {\"mask_token\": \"[MASK]\", \"cls_token\": \"[CLS]\", \"sep_token\": \"[SEP]\", \"pad_token\": \"[PAD]\"}\n",
    "tokenizer.add_special_tokens(special_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>app_id</th>\n",
       "      <th>mcc_cat_list</th>\n",
       "      <th>amnt_list_discrete</th>\n",
       "      <th>next_mcc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>878469</td>\n",
       "      <td>bbbbbbgbbeudnvbgfgffxxddvdffnaejjeajbgdjedqqal...</td>\n",
       "      <td>[8, 8, 8, 8, 8, 8, 2, 8, 8, 7, 8, 6, 5, 8, 8, ...</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>673064</td>\n",
       "      <td>bbbbbb</td>\n",
       "      <td>[6, 6, 6, 5, 6, 6]</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>978962</td>\n",
       "      <td>bbbbbbbadaadbeeaaabbbbbaeadaaabaavbbk</td>\n",
       "      <td>[8, 7, 6, 6, 6, 7, 4, 7, 5, 7, 4, 6, 6, 6, 6, ...</td>\n",
       "      <td>h</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>596073</td>\n",
       "      <td>bbbbbbbbggikieokfggaaaagaxfabafaaaabgggagbaaag...</td>\n",
       "      <td>[7, 7, 7, 5, 6, 6, 7, 8, 7, 3, 6, 8, 7, 4, 6, ...</td>\n",
       "      <td>j</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>528044</td>\n",
       "      <td>cbaabbbdcbcbibdccbcbbbbbsidgfccdbccbdgabbbdida...</td>\n",
       "      <td>[5, 8, 7, 6, 4, 1, 7, 2, 2, 4, 6, 6, 7, 8, 5, ...</td>\n",
       "      <td>c</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   app_id                                       mcc_cat_list  \\\n",
       "0  878469  bbbbbbgbbeudnvbgfgffxxddvdffnaejjeajbgdjedqqal...   \n",
       "1  673064                                             bbbbbb   \n",
       "2  978962              bbbbbbbadaadbeeaaabbbbbaeadaaabaavbbk   \n",
       "3  596073  bbbbbbbbggikieokfggaaaagaxfabafaaaabgggagbaaag...   \n",
       "4  528044  cbaabbbdcbcbibdccbcbbbbbsidgfccdbccbdgabbbdida...   \n",
       "\n",
       "                                  amnt_list_discrete next_mcc  \n",
       "0  [8, 8, 8, 8, 8, 8, 2, 8, 8, 7, 8, 6, 5, 8, 8, ...        a  \n",
       "1                                 [6, 6, 6, 5, 6, 6]        b  \n",
       "2  [8, 7, 6, 6, 6, 7, 4, 7, 5, 7, 4, 6, 6, 6, 6, ...        h  \n",
       "3  [7, 7, 7, 5, 6, 6, 7, 8, 7, 3, 6, 8, 7, 4, 6, ...        j  \n",
       "4  [5, 8, 7, 6, 4, 1, 7, 2, 2, 4, 6, 6, 7, 8, 5, ...        c  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_mcc_train0 = tokenizer(list(train_df.iloc[:250000].mcc_cat_list.values), return_tensors=\"pt\", max_length=256, \n",
    "                           padding=\"max_length\", truncation=True, padding_side=\"left\", return_token_type_ids=False)\n",
    "vectorized_mcc_train0, vectorized_mcc_train0_attnmask = vectorized_mcc_train0.input_ids.to(torch.int16), vectorized_mcc_train0.attention_mask.to(torch.bool)\n",
    "gc.collect()\n",
    "\n",
    "vectorized_mcc_train1 = tokenizer(list(train_df.iloc[250000:-250000].mcc_cat_list.values), return_tensors=\"pt\", max_length=256, \n",
    "                           padding=\"max_length\", truncation=True, padding_side=\"left\", return_token_type_ids=False)\n",
    "vectorized_mcc_train1, vectorized_mcc_train1_attnmask = vectorized_mcc_train1.input_ids.to(torch.int16), vectorized_mcc_train1.attention_mask.to(torch.bool)\n",
    "gc.collect()\n",
    "\n",
    "vectorized_mcc_train2 = tokenizer(list(train_df.iloc[-250000:].mcc_cat_list.values), return_tensors=\"pt\", max_length=256, \n",
    "                           padding=\"max_length\", truncation=True, padding_side=\"left\", return_token_type_ids=False)\n",
    "vectorized_mcc_train2, vectorized_mcc_train2_attnmask = vectorized_mcc_train2.input_ids.to(torch.int16), vectorized_mcc_train2.attention_mask.to(torch.bool)\n",
    "gc.collect()\n",
    "\n",
    "vectorized_mcc_test = tokenizer(list(test_df.mcc_cat_list.values), return_tensors=\"pt\", max_length=256, \n",
    "                           padding=\"max_length\", truncation=True, padding_side=\"left\")\n",
    "vectorized_mcc_test, vectorized_mcc_test_attnmask = vectorized_mcc_test.input_ids.to(torch.int16), vectorized_mcc_test.attention_mask.to(torch.bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_mcc_train = torch.concatenate([vectorized_mcc_train0, vectorized_mcc_train1, vectorized_mcc_train2], dim=0)\n",
    "vectorized_mcc_train_attnmask = torch.concatenate([vectorized_mcc_train0_attnmask, vectorized_mcc_train1_attnmask, vectorized_mcc_train2_attnmask], dim=0)\n",
    "\n",
    "assert vectorized_mcc_train.shape == vectorized_mcc_train_attnmask.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Построим Encoder (BERT-like) модель на задачу MLM. Т.к. нас по логике чуть больше интересуют токены ближе к концу (последние покупки) - учтем это в логике маскирования:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "PADTOKEN = 259\n",
    "\n",
    "def get_positional_encoding(seq_len, model_dim):\n",
    "    positions = torch.arange(seq_len).unsqueeze(1)\n",
    "    div_term = torch.exp(torch.arange(0, model_dim, 2) * -(np.log(10000.0) / model_dim))\n",
    "    positional_encodings = torch.zeros((seq_len, model_dim))\n",
    "    positional_encodings[:, 0::2] = torch.sin(positions * div_term)\n",
    "    positional_encodings[:, 1::2] = torch.cos(positions * div_term)\n",
    "    return positional_encodings\n",
    "\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, model_dim, num_heads, ff_dim, dropout=0.1):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=model_dim, num_heads=num_heads, dropout=dropout)\n",
    "        self.layer_norm1 = nn.LayerNorm(model_dim)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(model_dim, ff_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(ff_dim, model_dim)\n",
    "        )\n",
    "        self.layer_norm2 = nn.LayerNorm(model_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, attn_mask):\n",
    "        attn_output, _ = self.attention(query=x, key=x, value=x, key_padding_mask=attn_mask)\n",
    "        x = self.layer_norm1(x + self.dropout(attn_output))\n",
    "        ffn_output = self.ffn(x)\n",
    "        x = self.layer_norm2(x + self.dropout(ffn_output))\n",
    "        return x\n",
    "\n",
    "def mask_input(data, mask_token=256, pad_token=PADTOKEN, mask_prob=0.2, \n",
    "               orig_mask_proba_repl=0.1, random_mask_proba_repl=0.1,\n",
    "               last_tok_proba_repl=0.2):\n",
    "    pads = data != pad_token\n",
    "    mask = (torch.rand_like(data, dtype=torch.float) < mask_prob) & pads\n",
    "    masked_data = data.clone()\n",
    "    masked_data[mask] = mask_token\n",
    "    mask_indices = torch.nonzero(mask, as_tuple=True)\n",
    "    \n",
    "    orig_mask_indices = torch.rand(mask_indices[0].size(0)) < orig_mask_proba_repl\n",
    "    random_mask_indices = ~orig_mask_indices & (torch.rand(mask_indices[0].size(0)) < random_mask_proba_repl)\n",
    "    \n",
    "    masked_data[mask_indices[0][orig_mask_indices], mask_indices[1][orig_mask_indices]] = \\\n",
    "        data[mask_indices[0][orig_mask_indices], mask_indices[1][orig_mask_indices]]\n",
    "    \n",
    "    random_choices = data[pads]\n",
    "    random_choices = random_choices[torch.randint(len(random_choices), (random_mask_indices.sum().item(),))]\n",
    "    masked_data[mask_indices[0][random_mask_indices], mask_indices[1][random_mask_indices]] = random_choices\n",
    "\n",
    "    for i in range(data.size(0)):\n",
    "        row = data[i]\n",
    "        valid_indices = (row != pad_token).nonzero(as_tuple=True)[0]\n",
    "        if len(valid_indices) > 0:\n",
    "            last_idx = valid_indices[-1].item()\n",
    "            if torch.rand(1).item() < last_tok_proba_repl:\n",
    "                masked_data[i, last_idx] = mask_token\n",
    "                mask[i, last_idx] = True\n",
    "\n",
    "    return masked_data.int(), mask.bool()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поступим, как в тренировке roberta - будем делать MLM динамически каждую эпоху, а не один раз - для этого придется обновлять трейн датасет и даталоадер каждую эпоху соответственно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\"\n",
    "\n",
    "class TrxDataset(Dataset):\n",
    "    def __init__(self, \n",
    "                 vectorized_mcc_masked, vectorized_mcc_attnmask, vectorized_mcc_mlmmask, vectorized_mcc_target):\n",
    "\n",
    "        # X/model input\n",
    "        self.vectorized_mcc_masked = vectorized_mcc_masked.int().to(DEVICE)\n",
    "        self.vectorized_mcc_attnmask = ~vectorized_mcc_attnmask.bool().to(DEVICE)\n",
    "        self.vectorized_mcc_mlmmask = vectorized_mcc_mlmmask.bool().to(DEVICE) # на инференсе подаваться не будет\n",
    "        # Y/loss calc\n",
    "        self.vectorized_mcc_target = vectorized_mcc_target.int().to(DEVICE)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.vectorized_mcc_masked[idx], self.vectorized_mcc_attnmask[idx], self.vectorized_mcc_mlmmask[idx], self.vectorized_mcc_target[idx])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.vectorized_mcc_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(771040, 192760)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorized_mcc_train_masked, vectorized_mcc_train_mlmmask = mask_input(vectorized_mcc_train)\n",
    "vectorized_mcc_test_masked, vectorized_mcc_test_mlmmask = mask_input(vectorized_mcc_test)\n",
    "\n",
    "trainset = TrxDataset(\n",
    "    vectorized_mcc_train_masked, vectorized_mcc_train_attnmask, vectorized_mcc_train_mlmmask, vectorized_mcc_train\n",
    ")\n",
    "testset = TrxDataset(\n",
    "    vectorized_mcc_test_masked, vectorized_mcc_test_attnmask, vectorized_mcc_test_mlmmask, vectorized_mcc_test\n",
    ")\n",
    "\n",
    "len(trainset), len(testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.5, class_weights=None):\n",
    "        super(MaskedLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        # self.class_weights = torch.tensor(list(class_weights.values())).to(DEVICE) if class_weights else None\n",
    "        self.class_weights = None\n",
    "        self.criterion = nn.CrossEntropyLoss(reduction='none', weight=self.class_weights)\n",
    "\n",
    "    def forward(self, y_pred, y_true, mask):\n",
    "        loss = self.criterion(y_pred.transpose(1, 2), y_true)\n",
    "        attn_mask = (y_true != PADTOKEN).float()\n",
    "        if self.alpha == 0:\n",
    "            mask = mask * attn_mask\n",
    "            return (loss * mask).sum() / (mask.sum() + 1e-8) # lower computations\n",
    "        mask, unmask = mask * attn_mask, (1 - mask) * attn_mask\n",
    "        masked_loss = (loss * mask).sum() / (mask.sum() + 1e-8)\n",
    "        unmasked_loss = (loss * unmask).sum() / (unmask.sum() + 1e-8)\n",
    "        return (1 - self.alpha) * masked_loss + self.alpha * unmasked_loss\n",
    "\n",
    "class MaskedAccuracy:\n",
    "    @staticmethod\n",
    "    def compute(y_pred, y_true, mask):\n",
    "        preds = torch.argmax(y_pred, dim=-1)\n",
    "        correct = (preds == y_true).float() * mask\n",
    "        return correct.sum() / (mask.sum() + 1e-8)\n",
    "    \n",
    "class UnmaskedAccuracy:\n",
    "    @staticmethod\n",
    "    def compute(y_pred, y_true, mask):\n",
    "        preds = torch.argmax(y_pred, dim=-1)\n",
    "        correct = (preds == y_true).float() * (1 - mask) * (y_true != PADTOKEN).float()\n",
    "        total = ((1 - mask) * (y_true != PADTOKEN).float()).sum() + 1e-8\n",
    "        return correct.sum() / total\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, vocab_size, model_dim, num_heads, ff_dim, dropout, max_seq_len, num_layers):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.num_segments = 2\n",
    "        self.embedding = nn.Embedding(vocab_size, model_dim, padding_idx=PADTOKEN)\n",
    "        self.positional_encoding = get_positional_encoding(max_seq_len, model_dim)\n",
    "        self.encoders = nn.ModuleList(\n",
    "            [TransformerEncoder(model_dim, num_heads, ff_dim, dropout) for _ in range(num_layers)]\n",
    "        )\n",
    "        self.fc = nn.Linear(model_dim, vocab_size)\n",
    "        # self.register_buffer(\"positional_encoding\", get_positional_encoding(max_seq_len, model_dim))\n",
    "\n",
    "    def forward(self, x, attn_mask):\n",
    "        embeddings = self.embedding(x)\n",
    "        embeddings += self.positional_encoding[:embeddings.size(1), :].to(embeddings.device)\n",
    "        x = embeddings.permute(1, 0, 2)\n",
    "        for enc in self.encoders:\n",
    "            x = enc(x, attn_mask)\n",
    "        x = self.fc(x.permute(1, 0, 2))\n",
    "        return x\n",
    "    \n",
    "    def get_mlm_preds(self, x, mask, attn_mask):\n",
    "        x = self.forward(x, attn_mask)\n",
    "        return torch.argmax(x[(mask == 1)], dim=-1) \n",
    "\n",
    "    def get_hidden_states(self, x, attn_mask):\n",
    "        embeddings = self.embedding(x)\n",
    "        embeddings += self.positional_encoding[:embeddings.size(1), :].to(embeddings.device)\n",
    "        x = embeddings.permute(1, 0, 2)\n",
    "        for enc in self.encoders:\n",
    "            x = enc(x, attn_mask)\n",
    "        return x.permute(1, 0, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выполним небольшой грид-серч (насколько это позволяет инфраструктура относительно времени и производительности), логгирование будет производиться в локальный MLFlow сервер.\n",
    "\n",
    "```mlflow ui --host 0.0.0.0 --port 5000```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "combs = list(product(\n",
    "    [256, 512], #0 - dim\n",
    "    [4, 8], #1 - heads\n",
    "    [256, 512], #2 - ffdim\n",
    "    [.05], #3 - drp\n",
    "    [2, 4] #4 - num layers\n",
    "))\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "MAX_INPUT_LEN = 256\n",
    "\n",
    "trainloader = DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "valloader = DataLoader(testset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "for comb in tqdm(combs):\n",
    "    mlflow.set_tracking_uri(\"http://0.0.0.0:5000\")\n",
    "    mlflow.set_experiment(\"mlm_dynamic_trx_vtb\")\n",
    "    MODEL_DIM = comb[0]\n",
    "    NUM_HEADS = comb[1]\n",
    "    FF_DIM = comb[2]\n",
    "    DROPOUT = comb[3]\n",
    "    EPOCHS = 3\n",
    "    LEARNING_RATE = 1e-3\n",
    "    NUM_LAYERS = comb[4]\n",
    "    ALPHA_LOSS = 0 # FOR TRUE MLM\n",
    "    OTHER_NOTES = \"1st train\"\n",
    "\n",
    "    mlflow.log_params({\n",
    "        \"model_dim\": MODEL_DIM,\n",
    "        \"num_heads\": NUM_HEADS,\n",
    "        \"ff_dim\": FF_DIM,\n",
    "        \"dropout\": DROPOUT,\n",
    "        \"epochs\": EPOCHS,\n",
    "        \"learning_rate\": LEARNING_RATE,\n",
    "        \"num_layers\": NUM_LAYERS,\n",
    "        \"alpha_loss\": ALPHA_LOSS,\n",
    "        \"other_notes\": OTHER_NOTES,\n",
    "    })\n",
    "\n",
    "    trainloader = DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    valloader = DataLoader(testset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    vocab_size = len(tokenizer.get_vocab())\n",
    "    model = TransformerModel(vocab_size, MODEL_DIM, NUM_HEADS, FF_DIM, DROPOUT, MAX_INPUT_LEN, NUM_LAYERS)\n",
    "    model.to(DEVICE)\n",
    "    torch.cuda.empty_cache()\n",
    "    criterion = MaskedLoss(alpha=ALPHA_LOSS) #class_weights=weights_dict_norm)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer, max_lr=LEARNING_RATE, total_steps=len(trainloader) * EPOCHS\n",
    "    )\n",
    "    f1_metric = F1(num_classes=vocab_size, average='macro', task='multiclass').to(DEVICE)\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "\n",
    "        # roberta like\n",
    "        vectorized_mcc_train_masked, vectorized_mcc_train_mlmmask = mask_input(vectorized_mcc_train)\n",
    "        trainset = TrxDataset(\n",
    "            vectorized_mcc_train_masked, vectorized_mcc_train_attnmask, vectorized_mcc_train_mlmmask, vectorized_mcc_train\n",
    "        )\n",
    "        trainloader = DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "        model.train()\n",
    "        total_loss, total_acc, total_um_acc = 0, 0, 0\n",
    "        all_preds = []\n",
    "        all_targets = []\n",
    "        for vectorized_mcc_masked, vectorized_mcc_attnmask, vectorized_mcc_mlmmask, vectorized_mcc_target in tqdm(trainloader, desc=f\"Training Epoch {epoch+1}\"):\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(vectorized_mcc_masked, vectorized_mcc_attnmask)\n",
    "            loss = criterion(outputs, vectorized_mcc_target.long(), vectorized_mcc_mlmmask)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            total_loss += loss.item()\n",
    "            acc = MaskedAccuracy.compute(outputs, vectorized_mcc_target, vectorized_mcc_mlmmask.int())\n",
    "            acc_um = UnmaskedAccuracy.compute(outputs, vectorized_mcc_target, vectorized_mcc_mlmmask.int())\n",
    "            total_acc += acc.item()\n",
    "            total_um_acc += acc_um.item()\n",
    "            attn_mask = (vectorized_mcc_mlmmask == 1)\n",
    "            f1_preds = torch.argmax(outputs, dim=-1)[attn_mask]\n",
    "            f1_targets = vectorized_mcc_target[attn_mask]\n",
    "            all_preds.append(f1_preds)\n",
    "            all_targets.append(f1_targets)\n",
    "        all_preds = torch.cat(all_preds)\n",
    "        all_targets = torch.cat(all_targets)\n",
    "        f1_metric.reset()\n",
    "        f1_train = f1_metric(all_preds.to(DEVICE), all_targets.to(DEVICE))\n",
    "        train_loss_avg = total_loss / len(trainloader)\n",
    "        train_acc_avg = total_acc / len(trainloader)\n",
    "        train_um_acc_avg = total_um_acc / len(trainloader)\n",
    "        f1_train_avg = f1_train.item()\n",
    "        print(f\"[TRAIN] Epoch {epoch + 1}, Loss: {train_loss_avg:.4f}, Accuracy: {train_acc_avg:.4f}, \"\n",
    "            f\"Unmasked accuracy: {train_um_acc_avg:.4f}, F1-Macro: {f1_train_avg:.4f}\")\n",
    "        mlflow.log_metrics({\n",
    "            \"train_loss\": train_loss_avg,\n",
    "            \"train_accuracy\": train_acc_avg,\n",
    "            \"train_unmasked_accuracy\": train_um_acc_avg,\n",
    "            \"train_f1_macro\": f1_train_avg\n",
    "        }, step=epoch)\n",
    "\n",
    "\n",
    "        model.eval()\n",
    "        total_loss, total_acc, total_um_acc = 0, 0, 0\n",
    "        all_preds = []\n",
    "        all_targets = []\n",
    "        for vectorized_mcc_masked, vectorized_mcc_attnmask, vectorized_mcc_mlmmask, vectorized_mcc_target in tqdm(valloader, desc=f\"validating Epoch {epoch+1}\"):\n",
    "            outputs = model(vectorized_mcc_masked, vectorized_mcc_attnmask)\n",
    "            loss = criterion(outputs, vectorized_mcc_target.long(), vectorized_mcc_mlmmask)\n",
    "            total_loss += loss.item()\n",
    "            acc = MaskedAccuracy.compute(outputs, vectorized_mcc_target, vectorized_mcc_mlmmask.int())\n",
    "            acc_um = UnmaskedAccuracy.compute(outputs, vectorized_mcc_target, vectorized_mcc_mlmmask.int())\n",
    "            total_acc += acc.item()\n",
    "            total_um_acc += acc_um.item()\n",
    "            attn_mask = (vectorized_mcc_mlmmask == 1)\n",
    "            f1_preds = torch.argmax(outputs, dim=-1)[attn_mask]\n",
    "            f1_targets = vectorized_mcc_target[attn_mask]\n",
    "            all_preds.append(f1_preds)\n",
    "            all_targets.append(f1_targets)\n",
    "        all_preds = torch.cat(all_preds)\n",
    "        all_targets = torch.cat(all_targets)\n",
    "        f1_metric.reset()\n",
    "        f1_val = f1_metric(all_preds.to(DEVICE), all_targets.to(DEVICE))\n",
    "        val_loss_avg = total_loss / len(valloader)\n",
    "        val_acc_avg = total_acc / len(valloader)\n",
    "        val_um_acc_avg = total_um_acc / len(valloader)\n",
    "        f1_val_avg = f1_val.item()\n",
    "        print(f\"[val] Epoch {epoch + 1}, Loss: {val_loss_avg:.4f}, Accuracy: {val_acc_avg:.4f}, \"\n",
    "            f\"Unmasked accuracy: {val_um_acc_avg:.4f}, F1-Macro: {f1_val_avg:.4f}\")\n",
    "        mlflow.log_metrics({\n",
    "            \"val_loss\": val_loss_avg,\n",
    "            \"val_accuracy\": val_acc_avg,\n",
    "            \"val_unmasked_accuracy\": val_um_acc_avg,\n",
    "            \"val_f1_macro\": f1_val_avg\n",
    "        }, step=epoch)\n",
    "        \n",
    "\n",
    "    mlflow.end_run()\n",
    "    torch.save(model.state_dict(), f\"task3_recsys/models/mcc_encoder_L{NUM_LAYERS}_D{MODEL_DIM}_FF{FF_DIM}_H{NUM_HEADS}_MLM_0_2plus_fix.pth\")\n",
    "    # plus - доп. MLM на последний токен\n",
    "    # fix - после исправления бага\n",
    "    del model\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Лучшая модель на комбинациях выше - ```mcc_encoder_L2_D256_FF256_H4_MLM_0_2plus_fix.pth```\n",
    "\n",
    "---\n",
    "\n",
    "Далее - уберем \"голову\" и сделаем avg pooling по скрытым состояниям до этой головы. Это - эмбеддинги, которые далее мы будем использовать для предсказания категории последней транзакции (MCC). \n",
    "\n",
    "Перезапустим ноутбук, загрузим эту модель."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# from tokenizers.implementations import BertWordPieceTokenizer\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import gc\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "train_df = pd.read_parquet(\"task3_recsys/df_for_transformer_TRAIN.parquet\")\n",
    "test_df = pd.read_parquet(\"task3_recsys/df_for_transformer_TEST.parquet\")\n",
    "\n",
    "tokenizer = PreTrainedTokenizerFast(tokenizer_file=\"task3_recsys/models/tokenizer/vocab_256_bpe.json\") # наиболее адекватное распределение по сравнению с большим кол-вом токенов\n",
    "special_tokens = {\"mask_token\": \"[MASK]\", \"cls_token\": \"[CLS]\", \"sep_token\": \"[SEP]\", \"pad_token\": \"[PAD]\"}\n",
    "tokenizer.add_special_tokens(special_tokens)\n",
    "\n",
    "vectorized_mcc_train0 = tokenizer(list(train_df.iloc[:250000].mcc_cat_list.values), return_tensors=\"pt\", max_length=256, \n",
    "                           padding=\"max_length\", truncation=True, padding_side=\"left\", return_token_type_ids=False)\n",
    "vectorized_mcc_train0, vectorized_mcc_train0_attnmask = vectorized_mcc_train0.input_ids.to(torch.int16), vectorized_mcc_train0.attention_mask.to(torch.bool)\n",
    "gc.collect()\n",
    "\n",
    "vectorized_mcc_train1 = tokenizer(list(train_df.iloc[250000:-250000].mcc_cat_list.values), return_tensors=\"pt\", max_length=256, \n",
    "                           padding=\"max_length\", truncation=True, padding_side=\"left\", return_token_type_ids=False)\n",
    "vectorized_mcc_train1, vectorized_mcc_train1_attnmask = vectorized_mcc_train1.input_ids.to(torch.int16), vectorized_mcc_train1.attention_mask.to(torch.bool)\n",
    "gc.collect()\n",
    "\n",
    "vectorized_mcc_train2 = tokenizer(list(train_df.iloc[-250000:].mcc_cat_list.values), return_tensors=\"pt\", max_length=256, \n",
    "                           padding=\"max_length\", truncation=True, padding_side=\"left\", return_token_type_ids=False)\n",
    "vectorized_mcc_train2, vectorized_mcc_train2_attnmask = vectorized_mcc_train2.input_ids.to(torch.int16), vectorized_mcc_train2.attention_mask.to(torch.bool)\n",
    "gc.collect()\n",
    "\n",
    "vectorized_mcc_test = tokenizer(list(test_df.mcc_cat_list.values), return_tensors=\"pt\", max_length=256, \n",
    "                           padding=\"max_length\", truncation=True, padding_side=\"left\")\n",
    "vectorized_mcc_test, vectorized_mcc_test_attnmask = vectorized_mcc_test.input_ids.to(torch.int16), vectorized_mcc_test.attention_mask.to(torch.bool)\n",
    "\n",
    "vectorized_mcc_train = torch.concatenate([vectorized_mcc_train0, vectorized_mcc_train1, vectorized_mcc_train2], dim=0)\n",
    "vectorized_mcc_train_attnmask = torch.concatenate([vectorized_mcc_train0_attnmask, vectorized_mcc_train1_attnmask, vectorized_mcc_train2_attnmask], dim=0)\n",
    "\n",
    "assert vectorized_mcc_train.shape == vectorized_mcc_train_attnmask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert tokenizer.get_vocab()[\"[PAD]\"] == 259, \"padding not aligned\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Re-declare всех нужных классов и функций для инференса:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PADTOKEN = 259\n",
    "\n",
    "def get_positional_encoding(seq_len, model_dim):\n",
    "    positions = torch.arange(seq_len).unsqueeze(1)\n",
    "    div_term = torch.exp(torch.arange(0, model_dim, 2) * -(np.log(10000.0) / model_dim))\n",
    "    positional_encodings = torch.zeros((seq_len, model_dim))\n",
    "    positional_encodings[:, 0::2] = torch.sin(positions * div_term)\n",
    "    positional_encodings[:, 1::2] = torch.cos(positions * div_term)\n",
    "    return positional_encodings\n",
    "\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, model_dim, num_heads, ff_dim, dropout=0.1):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=model_dim, num_heads=num_heads, dropout=dropout)\n",
    "        self.layer_norm1 = nn.LayerNorm(model_dim)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(model_dim, ff_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(ff_dim, model_dim)\n",
    "        )\n",
    "        self.layer_norm2 = nn.LayerNorm(model_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, attn_mask):\n",
    "        attn_output, _ = self.attention(query=x, key=x, value=x, key_padding_mask=attn_mask)\n",
    "        x = self.layer_norm1(x + self.dropout(attn_output))\n",
    "        ffn_output = self.ffn(x)\n",
    "        x = self.layer_norm2(x + self.dropout(ffn_output))\n",
    "        return x\n",
    "    \n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, vocab_size, model_dim, num_heads, ff_dim, dropout, max_seq_len, num_layers):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.num_segments = 2\n",
    "        self.embedding = nn.Embedding(vocab_size, model_dim, padding_idx=PADTOKEN)\n",
    "        self.positional_encoding = get_positional_encoding(max_seq_len, model_dim)\n",
    "        self.encoders = nn.ModuleList(\n",
    "            [TransformerEncoder(model_dim, num_heads, ff_dim, dropout) for _ in range(num_layers)]\n",
    "        )\n",
    "        self.fc = nn.Linear(model_dim, vocab_size)\n",
    "        # self.register_buffer(\"positional_encoding\", get_positional_encoding(max_seq_len, model_dim))\n",
    "\n",
    "    def forward(self, x, attn_mask):\n",
    "        embeddings = self.embedding(x)\n",
    "        embeddings += self.positional_encoding[:embeddings.size(1), :].to(embeddings.device)\n",
    "        x = embeddings.permute(1, 0, 2)\n",
    "        for enc in self.encoders:\n",
    "            x = enc(x, attn_mask)\n",
    "        x = self.fc(x.permute(1, 0, 2))\n",
    "        return x\n",
    "    \n",
    "    def get_mlm_preds(self, x, mask, attn_mask):\n",
    "        x = self.forward(x, attn_mask)\n",
    "        return torch.argmax(x[(mask == 1)], dim=-1) \n",
    "\n",
    "    def get_hidden_states(self, x, attn_mask):\n",
    "        embeddings = self.embedding(x)\n",
    "        embeddings += self.positional_encoding[:embeddings.size(1), :].to(embeddings.device)\n",
    "        x = embeddings.permute(1, 0, 2)\n",
    "        for enc in self.encoders:\n",
    "            x = enc(x, attn_mask)\n",
    "        return x.permute(1, 0, 2)\n",
    "\n",
    "class EmbedTransformerModel(nn.Module):\n",
    "    def __init__(self, transformer_model):\n",
    "        super(EmbedTransformerModel, self).__init__()\n",
    "        self.transformer = transformer_model\n",
    "\n",
    "    def forward(self, x):\n",
    "        inputs, attn_mask = x[0].int(), x[1].to(bool)\n",
    "        inputs, attn_mask = inputs.to(DEVICE), attn_mask.to(DEVICE).to(DEVICE)\n",
    "        hidden_states = self.transformer.get_hidden_states(inputs, attn_mask)  # (batch_size, seq_len, model_dim)\n",
    "        attention_mask = (attn_mask != 1).unsqueeze(-1)  # (batch_size, seq_len, 1)\n",
    "        masked_hidden_states = hidden_states * attention_mask  # apply mask\n",
    "        avg_pooled_embeddings = masked_hidden_states.sum(dim=1) / (attention_mask.sum(dim=1) + 1e-8)\n",
    "        return avg_pooled_embeddings\n",
    "        \n",
    "MODEL_DIM = 256\n",
    "NUM_HEADS = 4\n",
    "FF_DIM = 256\n",
    "DROPOUT = .05\n",
    "NUM_LAYERS = 2\n",
    "MAX_INPUT_LEN = 256\n",
    "DEVICE = \"cuda\"\n",
    "    \n",
    "model = TransformerModel(len(tokenizer.get_vocab()), MODEL_DIM, NUM_HEADS, FF_DIM, DROPOUT, MAX_INPUT_LEN, NUM_LAYERS)\n",
    "model.load_state_dict(torch.load(\"task3_recsys/models/mcc_encoder_L2_D256_FF256_H4_MLM_0_2plus.pth\", weights_only=True, map_location=DEVICE))\n",
    "model.to(DEVICE)\n",
    "    \n",
    "embed_model = EmbedTransformerModel(model)\n",
    "embed_model.to(DEVICE)\n",
    "embed_model.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Будем смотреть результат и финальные метрики на тест-части и эмбедить ее же. Но оптимизировать саму задачу рекомендации следующего MCC нужно на трейн части, поэтому эмбедим ее тоже."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "\n",
    "trainset = TensorDataset(vectorized_mcc_train, ~vectorized_mcc_train_attnmask)\n",
    "trainloader = DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "valset = TensorDataset(vectorized_mcc_test, ~vectorized_mcc_test_attnmask)\n",
    "valloader = DataLoader(valset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 48190/48190 [01:36<00:00, 497.43it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 12048/12048 [00:23<00:00, 502.15it/s]\n"
     ]
    }
   ],
   "source": [
    "train_embs = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for c, batch in enumerate(tqdm(trainloader)):\n",
    "        avg_pooled_embeddings = embed_model(batch)\n",
    "        embs_ = avg_pooled_embeddings.cpu().detach().numpy()\n",
    "        train_embs.append(embs_)\n",
    "\n",
    "train_embs = np.concatenate(train_embs)\n",
    "\n",
    "val_embs = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for c, batch in enumerate(tqdm(valloader)):\n",
    "        avg_pooled_embeddings = embed_model(batch)\n",
    "        embs_ = avg_pooled_embeddings.cpu().detach().numpy()\n",
    "        val_embs.append(embs_)\n",
    "\n",
    "val_embs = np.concatenate(val_embs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_train_df = train_df.join(pd.DataFrame(train_embs))\n",
    "final_test_df = test_df.join(pd.DataFrame(val_embs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/roman/.local/lib/python3.12/site-packages/pandas/io/parquet.py:190: UserWarning: The DataFrame has column names of mixed type. They will be converted to strings and not roundtrip correctly.\n",
      "  table = self.api.Table.from_pandas(df, **from_pandas_kwargs)\n"
     ]
    }
   ],
   "source": [
    "final_train_df.to_parquet(\"task3_recsys/train_df_after_transformer.parquet\")\n",
    "final_test_df.to_parquet(\"task3_recsys/test_df_after_transformer.parquet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
